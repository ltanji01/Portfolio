{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier for Images - Duplicate Images ONLY\n",
    "Develop an MLP with at most 1 hidden layer that can solve the 6-class fashion problem as well as possible. The training data is heavily skewed with data from sandals and trainers, with only one training image for 2 classes. The challenge will be gaining a balanced weighting for each of the classes such that the dominant classes aren't always favoured by the classifier.\n",
    "\n",
    "Stipulations:\n",
    "* Use an MLPClassifier with at most one hidden layer (but any number of units)\n",
    "* Only use the provided train data for model fitting. No augmentation/duplication allowed here.\n",
    "* Only use the provided valid data for hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline:\n",
    "* [Part 1: Duplicate Data](#part1)\n",
    "* [Part 2: Coarse Grid Search](#part2)\n",
    "* [Part 3: Extract Best Parameters](#part3)\n",
    "* [Part 4: Recreate Best Model](#part4)\n",
    "* [Part 5: Fine Grid Search over Each Parameter](#part5)\n",
    "* [Part 6: Final Best Model](#part6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File handling\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# General functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sci-kit learn\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, PredefinedSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom functions\n",
    "from load_data import load_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data:\n",
    "# x data is an array of N*784 pixels (N = 2102 for tr, 600 for va)\n",
    "# y is a dataframe of index, class_name and class_id\n",
    "x_tr, y_tr_df = load_data('x_train.csv', 'y_train.csv')\n",
    "x_va, y_va_df = load_data('x_valid.csv', 'y_valid.csv')\n",
    "x_te = load_data('x_test.csv', 'y_valid.csv')[0]\n",
    "\n",
    "\n",
    "for label, arr in [('train', x_tr), ('valid', x_va)]:\n",
    "    print(\"Contents of %s_x.csv: arr of shape %s\" % (\n",
    "        label, str(arr.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "# Duplicate Data\n",
    "The given training set only has 1 top and trouser image, 400 dresses, 100 pullovers, and 800 sandals and sneakers. To even out the classes we're going to duplicate each image to have 800 images of each class. At the end, we should have a total of 4800 images in the entire training set. \n",
    "\n",
    "- Add 800 copies of the top and trouser.\n",
    "- Make a copy of each dress to total 800 dresses. \n",
    "- Add 8 copies of each of the 100 sneakers.\n",
    "\n",
    "We're also defining a custom splitter so that the validation set is not used for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting index for each class in training set:\n",
    "top_int = y_tr_df.index[y_tr_df['class_name']=='top']    #index 1\n",
    "trous_int = y_tr_df.index[y_tr_df['class_name']=='trouser']    #index 11\n",
    "dress_int =(y_tr_df.index[y_tr_df['class_name']=='dress']).tolist()\n",
    "pull_int =(y_tr_df.index[y_tr_df['class_name']=='pullover']).tolist()\n",
    "sneaker_int =(y_tr_df.index[y_tr_df['class_name']=='sneaker']).tolist()\n",
    "sandal_int =(y_tr_df.index[y_tr_df['class_name']=='sandal']).tolist()\n",
    "\n",
    "\n",
    "#sneakers + sandals + tops + trous + dress + pullovers in new dataset.\n",
    "train_x_2 = np.vstack((x_tr[sandal_int, :], x_tr[sneaker_int, :]))\n",
    "tops800= np.tile(x_tr[top_int, :], (800, 1)) #make 800 copies of tops\n",
    "train_x_2 = np.vstack((train_x_2, tops800)) # add 800 to sandals + sneakers\n",
    "assert(train_x_2.shape == (2400, 784))  #checking dimensions\n",
    "\n",
    "trous800 = np.tile(x_tr[trous_int, :], (800, 1))  #800 copies of trousers\n",
    "train_x_2 = np.vstack((train_x_2, trous800)) # add 800 trousers\n",
    "assert(train_x_2.shape == (3200, 784))  #checking dimensions\n",
    "\n",
    "dress800 = np.tile(x_tr[dress_int, :], (2, 1))   #doubled dresses 400 to 800\n",
    "assert(dress800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, dress800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4000, 784))  #checking dimensions\n",
    "\n",
    "pull800 = np.tile(x_tr[pull_int, :], (8, 1))\n",
    "assert(pull800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, pull800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4800, 784))\n",
    "\n",
    "#making train_y_2\n",
    "train_y_2 = np.tile(5, 800)\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(7, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(0, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(1, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(3, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(2, 800)))\n",
    "\n",
    "assert(train_y_2.shape == (4800, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape: (4800, 784)\n",
      "Validation X shape: (600, 784)\n",
      "Combined X shape: (5400, 784)\n",
      "\n",
      "Training Y shape: (4800,)\n",
      "Validation Y shape: (600,)\n",
      "Combined Y shape: (5400,)\n",
      "\n",
      "Splitter dimensions: 5400\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation datasets\n",
    "x_dup_all = np.vstack((train_x_2, x_va))\n",
    "y_dup_all = np.hstack((train_y_2, y_va_df['class_uid']))\n",
    "\n",
    "print(\"Training X shape: %s\\nValidation X shape: %s\\nCombined X shape: %s\\n\" % (train_x_2.shape, x_va.shape, x_dup_all.shape))\n",
    "print(\"Training Y shape: %s\\nValidation Y shape: %s\\nCombined Y shape: %s\\n\" % (train_y_2.shape, y_va_df['class_uid'].shape, y_dup_all.shape))\n",
    "\n",
    "\n",
    "# Create a PredefinedSplit object to use for cross-validation\n",
    "valid_dup_indicators = np.hstack([\n",
    "    -1 * np.ones(train_y_2.shape[0]), #-1 = exclude this example in test split\n",
    "    0 * np.ones(y_va_df.shape[0]), #0 = include in the first test split\n",
    "    ])\n",
    "\n",
    "\n",
    "print(\"Splitter dimensions: %i\" % (valid_dup_indicators.shape[0]))\n",
    "my_dup_splitter = sklearn.model_selection.PredefinedSplit(valid_dup_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b id='part2'></b>\n",
    "# Coarse Grid search\n",
    "Creating our first MLP classifier with at most 1 hidden layer. We're using lbfgs as the solver since we have a smaller training set. Also, there's fewer hyperparameters to tune compared to other solvers.\n",
    "\n",
    "For the first search, we're trying the following parameters:\n",
    "- hidden layer sizes (10,),(20,),(50,),(100,)\n",
    "- activation function: identity and relu\n",
    "- max iterations: 10,  20,  43,  91, 190, 398\n",
    "- apha: 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = False\n",
    "flag = 'stop'\n",
    "\n",
    "if scaling:\n",
    "    filename = '1D_coarse_grid_search.sav'\n",
    "else:\n",
    "    filename = '1D_coarse_grid_searchNoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    grid_1D_model = pickle.load(open(filename, 'rb'))\n",
    "else:\n",
    "    rand_param_dist = dict(hidden_layer_sizes=[(10,),(20,),(50,),(100,)], activation=('identity', 'relu'), max_iter = np.logspace(1,2.6,6,dtype=int), alpha = np.logspace(-5,3,9))\n",
    "    \n",
    "    if scaling:\n",
    "        grid_1D_model =   sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLPClassifier(solver='lbfgs', random_state=0), rand_param_dist, scoring='balanced_accuracy', error_score='raise', return_train_score=True, cv= my_dup_splitter, n_jobs = -1, refit= False))\n",
    "        ])\n",
    "    else:\n",
    "        grid_1D_model =   sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLPClassifier(solver='lbfgs', random_state=0), rand_param_dist, scoring='balanced_accuracy', error_score='raise', return_train_score=True, cv= my_dup_splitter, n_jobs = -1, refit= False))\n",
    "        ])\n",
    "\n",
    "    # Fit on x_all as the custom splitter will divide this into tr and val\n",
    "    grid_1D_model.fit(x_dup_all, y_dup_all)\n",
    "\n",
    "    # Pickling the model since each one takes a while to run.\n",
    "    pickle.dump(grid_1D_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7600000000000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1DRes = grid_1D_model['grid_search']\n",
    "grid1DRes.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<c id='part3'></c>\n",
    "# Extract best parameters:\n",
    "When refit is true we can use the best_estimator_ method, but this doesn't work with refit=false - here we need to manually run a fit on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 418 with Balanced Accuracy: 0.760000\n",
      "Alpha: 1000.000000\n",
      "Layer: (20,)\n",
      "Activation: relu\n",
      "Max_iter: 190\n"
     ]
    }
   ],
   "source": [
    "# When refit is true we can use the best_estimator_ method, but this doesn't work with refit=false - here we need to manually run a fit on our training set.\n",
    "gridRes = grid_1D_model['grid_search'].cv_results_\n",
    "bestIdx = grid_1D_model['grid_search'].best_index_ \n",
    "\n",
    "testScore = gridRes['mean_test_score']\n",
    "\n",
    "bestAlpha = gridRes['params'][bestIdx]['alpha']\n",
    "bestLayer = gridRes['params'][bestIdx]['hidden_layer_sizes']\n",
    "bestActivation = gridRes['params'][bestIdx]['activation']\n",
    "bestMaxIter = gridRes['params'][bestIdx]['max_iter']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx,testScore[bestIdx],bestAlpha,str(bestLayer),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<d id='part4'></d>\n",
    "# Recreate the best model:\n",
    "Using the best hyperparameters found through the coarse grid search, we recreate the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training balanced accuracy: 0.666250\n",
      "Validation balanced accuracy: 0.638333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# Model with params from best index\n",
    "if scaling:\n",
    "        bestMLP1Dgrid =   sklearn.pipeline.Pipeline([\n",
    "        ('scaling', MinMaxScaler()),\n",
    "        ('MLP', MLPClassifier(solver='lbfgs', random_state=0, activation=bestActivation, alpha=bestAlpha, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter))])\n",
    "else:\n",
    "        bestMLP1Dgrid =   sklearn.pipeline.Pipeline([\n",
    "        ('MLP', MLPClassifier(solver='lbfgs', random_state=0, activation=bestActivation, alpha=bestAlpha, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter))])\n",
    "\n",
    "bestMLP1Dgrid.fit(x_tr,y_tr_df[\"class_name\"])\n",
    "\n",
    "pred_tr = bestMLP1Dgrid.predict(x_tr)\n",
    "pred_va = bestMLP1Dgrid.predict(x_va)\n",
    "pred_te = bestMLP1Dgrid.predict(x_te)\n",
    "\n",
    "tr_acc = balanced_accuracy_score(y_tr_df['class_name'], pred_tr)\n",
    "va_acc = balanced_accuracy_score(y_va_df['class_name'], pred_va)\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))\n",
    "\n",
    "\n",
    "# # # Save output of prediction on test data to a file.\n",
    "# np.savetxt('yhat_test.txt', pred_te, delimiter='\\n', fmt='%s');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<e id='part5'></e>\n",
    "# Fine grid search over each hyperparameter:\n",
    "Using the best initial parameters from before, do a fine search of eac hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate alpha \n",
    "Trying alpha = np.logspace(-3,5,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 260 with Balanced Accuracy: 0.803333\n",
      "Alpha: 100000.000000\n",
      "Layer: (20,)\n",
      "Activation: relu\n",
      "Max_iter: 190\n"
     ]
    }
   ],
   "source": [
    "flag = 'stop'\n",
    "\n",
    "if scaling:\n",
    "    filename = '1D_alphaGrid_search.sav'\n",
    "else:\n",
    "    filename = '1D_alphaGrid_NoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    evalHypeParamAlphaPipe = pickle.load(open(filename, 'rb'))\n",
    "    alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "    alphaDist = alphaRes['param_alpha']\n",
    "else:\n",
    "    ### Evaluate alpha!\n",
    "\n",
    "    # Create a MLP classifier\n",
    "    MLP1D_EvalHype =   MLPClassifier(solver='lbfgs', activation=bestActivation, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter)\n",
    "    # Hyperparameters distributions - regularization strength alpha\n",
    "\n",
    "    alphaDist = np.logspace(-3,5,17)\n",
    "    randStateDist = range(0,16,1)\n",
    "    distEvalHype = dict(alpha = alphaDist, random_state=randStateDist) #[10**(-4), optParams['C'], 10**6])\n",
    "\n",
    "    #Pipeline starts!\n",
    "    if scaling:\n",
    "        evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "        ])\n",
    "    else:\n",
    "        evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "        ])\n",
    "\n",
    "    evalHypeParamAlphaPipe.fit(x_dup_all, y_dup_all)    \n",
    "    pickle.dump(evalHypeParamAlphaPipe, open(filename, 'wb'))\n",
    "\n",
    "    alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "bestIdx2 = evalHypeParamAlphaPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = alphaRes['mean_test_score']\n",
    "\n",
    "bestAlpha2 = alphaRes['params'][bestIdx2]['alpha']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx2,testScore[bestIdx2],bestAlpha2,str(bestLayer),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate num units in layer \n",
    "Other values are from best estimator and previous alpha search.\n",
    "\n",
    "Trying layer sizes: (1,),(6,),(8,),(10,),(12,),(15,),(20,),(24,),(30,),(40,),(50,),(75,),(100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 52 with Balanced Accuracy: 0.811667\n",
      "Alpha: 100000.000000\n",
      "Layer: (10,)\n",
      "Activation: relu\n",
      "Max_iter: 190\n"
     ]
    }
   ],
   "source": [
    "flag = 'stop'\n",
    "\n",
    "if scaling:\n",
    "    filename = '1D_layerGrid_search.sav'\n",
    "else:\n",
    "    filename = '1D_layerGrid_NoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    evalHypeParamLayerPipe = pickle.load(open(filename, 'rb'))\n",
    "else:\n",
    "    ### Evaluate Number of Units!\n",
    "\n",
    "    # Create a MLP classifier\n",
    "    MLP1D_EvalHype = MLPClassifier(solver='lbfgs', activation=bestActivation, alpha=bestAlpha2, max_iter=bestMaxIter)\n",
    "    # Hyperparameters distributions - regularization strength alpha\n",
    "\n",
    "    layerDist = [(1,),(6,),(8,),(10,),(12,),(15,),(20,),(24,),(30,),(40,),(50,),(75,),(100,)]\n",
    "\n",
    "    randStateDist = range(0,16,1)\n",
    "    distEvalHype = dict(hidden_layer_sizes = layerDist, random_state=randStateDist)\n",
    "\n",
    "    #Pipeline starts!\n",
    "    if scaling:\n",
    "        evalHypeParamLayerPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "    else:\n",
    "        evalHypeParamLayerPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "\n",
    "    evalHypeParamLayerPipe.fit(x_dup_all, y_dup_all)\n",
    "    pickle.dump(evalHypeParamLayerPipe, open(filename, 'wb'))\n",
    "\n",
    "layerRes = evalHypeParamLayerPipe['grid_search'].cv_results_\n",
    "bestIdx3 = evalHypeParamLayerPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = layerRes['mean_test_score']\n",
    "\n",
    "bestLayer3 = layerRes['params'][bestIdx3]['hidden_layer_sizes']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx3,testScore[bestIdx3],bestAlpha2,str(bestLayer3),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate max iter \n",
    "Trying following values for max Iter: 1, 2, 3, 5, 8, 13, 19, 30, 46, 71, 110, 169, 259, 398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 145 with Balanced Accuracy: 0.836667\n",
      "Alpha: 100000.000000\n",
      "Layer: (10,)\n",
      "Activation: relu\n",
      "Max_iter: 46\n",
      "Rand state: 1\n"
     ]
    }
   ],
   "source": [
    "flag = 'stop'\n",
    "scaling = False\n",
    "if scaling:\n",
    "    filename = '1D_iterGrid_search.sav'\n",
    "else:\n",
    "    filename = '1D_iterGrid_NoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    evalHypeParamIterPipe = pickle.load(open(filename, 'rb'))\n",
    "else:\n",
    "    ### Evaluate Max_iter (i.e. early stopping for lbfgs)!\n",
    "\n",
    "    # Create a MLP classifier\n",
    "    MLP1D_EvalHype = MLPClassifier(solver='lbfgs', activation=bestActivation, alpha=bestAlpha2, hidden_layer_sizes = bestLayer3)\n",
    "    # Hyperparameters distributions - max iterations\n",
    "    max_iterDist = np.logspace(0,2.6,15,dtype=int)\n",
    "\n",
    "    randStateDist = range(0,16,1)\n",
    "    distEvalHype = dict(max_iter = max_iterDist, random_state=randStateDist)\n",
    "\n",
    "    #Pipeline starts!s\n",
    "    if scaling:\n",
    "        evalHypeParamIterPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "    else:\n",
    "        evalHypeParamIterPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "\n",
    "    evalHypeParamIterPipe.fit(x_dup_all, y_dup_all)\n",
    "    pickle.dump(evalHypeParamIterPipe, open(filename, 'wb'))\n",
    "\n",
    "iterRes = evalHypeParamIterPipe['grid_search'].cv_results_\n",
    "bestIdx4 = evalHypeParamIterPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = iterRes['mean_test_score']\n",
    "bestIter4 = iterRes['params'][bestIdx4]['max_iter']\n",
    "randState4 = iterRes['params'][bestIdx4]['random_state']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\\nRand state: %i\" % (bestIdx4,testScore[bestIdx4],bestAlpha2,str(bestLayer3),bestActivation,bestIter4,randState4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<f id='part6'></f>\n",
    "# Final Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training balanced accuracy: 0.492083\n",
      "Validation balanced accuracy: 0.478333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# Create the final Best MLP classifier for 1D\n",
    "MLP1D_Final = MLPClassifier(solver='lbfgs', activation=bestActivation, alpha=bestAlpha2, hidden_layer_sizes = bestLayer3, max_iter=bestIter4, random_state=randState4)\n",
    "\n",
    "MLP1D_Final.fit(x_tr,y_tr_df['class_uid'])\n",
    "\n",
    "pred_tr = MLP1D_Final.predict(x_tr)\n",
    "pred_va = MLP1D_Final.predict(x_va)\n",
    "pred_te = MLP1D_Final.predict(x_te)\n",
    "\n",
    "tr_acc = balanced_accuracy_score(y_tr_df['class_uid'], pred_tr)\n",
    "va_acc = balanced_accuracy_score(y_va_df['class_uid'], pred_va)\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Alpha data for plotting\n",
    "# alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "# reshape to rows of one alpha, with columns of different random seeds\n",
    "alphaScoreTrainReshape = alphaRes['mean_train_score'].reshape((len(alphaDist),16))\n",
    "alphaScoreValReshape = alphaRes['mean_test_score'].reshape((len(alphaDist),16))\n",
    "\n",
    "# Select best runs for each alpha\n",
    "alpha_Train = np.max(alphaScoreTrainReshape,axis=1)\n",
    "alpha_Val = np.max(alphaScoreValReshape,axis=1)\n",
    "# Make second alphaDist to be used for scatter plot (i.e. values repeated for each rand state)\n",
    "alphaDistRep = alphaRes['param_alpha']\n",
    "\n",
    "# Extract layer data for plotting\n",
    "layerRes = evalHypeParamLayerPipe['grid_search'].cv_results_\n",
    "# reshape to rows of one layer size, with columns of different random seeds\n",
    "layerScoreTrainReshape = layerRes['mean_train_score'].reshape((len(layerDist),16))\n",
    "layerScoreValReshape = layerRes['mean_test_score'].reshape((len(layerDist),16))\n",
    "\n",
    "# Select best runs for each layer size\n",
    "layer_Train = np.max(layerScoreTrainReshape,axis=1)\n",
    "layer_Val = np.max(layerScoreValReshape,axis=1)\n",
    "# Make second layerDist to be used for scatter plot (i.e. values repeated for each rand state)\n",
    "layerDistUnpack = [e for e, in layerDist]\n",
    "layerDistRep = np.repeat(layerDistUnpack,16)\n",
    "\n",
    "# # Extract max_iter data for plotting\n",
    "# IterRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "# reshape to rows of one max_iter, with columns of different random seeds\n",
    "iterScoreTrainReshape = iterRes['mean_train_score'].reshape((len(max_iterDist),16))\n",
    "iterScoreValReshape = iterRes['mean_test_score'].reshape((len(max_iterDist),16))\n",
    "\n",
    "# Select best runs for each iter\n",
    "iter_Train = np.max(iterScoreTrainReshape,axis=1)\n",
    "iter_Val = np.max(iterScoreValReshape,axis=1)\n",
    "# Make second iterDist to be used for scatter plot (i.e. values repeated for each rand state)\n",
    "iterDistRep = iterRes['param_max_iter']\n",
    "\n",
    "\n",
    "# Plot accuracy vs alpha, layer and max_iter hyperparameters\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(10,12))\n",
    "ax1.scatter(alphaDistRep, alphaRes['mean_train_score'], label='Training Balanced Accuracy',marker='.')\n",
    "ax1.scatter(alphaDistRep, alphaRes['mean_test_score'], label='Validation Balanced Accuracy',marker='.')\n",
    "ax1.plot(alphaDist, alpha_Train, label='Best Training Balanced Accuracy')\n",
    "ax1.plot(alphaDist, alpha_Val, label='Best Validation Balanced Accuracy')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Alpha Penalization')\n",
    "ax1.set_ylabel('Balanced Accuracy')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(which='both')\n",
    "\n",
    "ax2.scatter(layerDistRep, layerRes['mean_train_score'], label='Training Balanced Accuracy',marker='.')\n",
    "ax2.scatter(layerDistRep, layerRes['mean_test_score'], label='Validation Balanced Accuracy',marker='.')\n",
    "ax2.plot(layerDistUnpack, layer_Train, label='Best Training Balanced Accuracy')\n",
    "ax2.plot(layerDistUnpack, layer_Val, label='Best Validation Balanced Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Units in Hidden Layer')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(which='both')\n",
    "\n",
    "ax3.scatter(iterDistRep, iterRes['mean_train_score'], label='Training Balanced Accuracy',marker='.')\n",
    "ax3.scatter(iterDistRep, iterRes['mean_test_score'], label='Validation Balanced Accuracy',marker='.')\n",
    "ax3.plot(max_iterDist, iter_Train, label='Best Training Balanced Accuracy')\n",
    "ax3.plot(max_iterDist, iter_Val, label='Best Validation Balanced Accuracy')\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('Max Iter')\n",
    "ax3.set_ylabel('Balanced Accuracy')\n",
    "ax3.set_xscale('log')\n",
    "ax3.grid(which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.ConfusionMatrixDisplay.from_estimator(evalHypeParamAlphaPipe['class_uid'], x_va, y_va_df['class_uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP1D_EvalHype =   MLPClassifier(solver='lbfgs', activation=bestActivation, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter)\n",
    "# Hyperparameters distributions - regularization strength alpha\n",
    "\n",
    "alphaDist = np.logspace(-1.5,5,17)\n",
    "randStateDist = range(0,16,1)\n",
    "distEvalHype = dict(alpha = alphaDist, random_state=randStateDist) #[10**(-4), optParams['C'], 10**6])\n",
    "\n",
    "#Pipeline starts!\n",
    "if scaling:\n",
    "    evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "        ('scaling', MinMaxScaler()),\n",
    "        ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "    ])\n",
    "else:\n",
    "    evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "        ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "    ])\n",
    "\n",
    "evalHypeParamAlphaPipe.fit(x_dup_all, y_dup_all)    \n",
    "pickle.dump(evalHypeParamAlphaPipe, open(filename, 'wb'))\n",
    "\n",
    "alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "bestIdx2 = evalHypeParamAlphaPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = alphaRes['mean_test_score']\n",
    "\n",
    "bestAlpha2 = alphaRes['params'][bestIdx2]['alpha']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx2,testScore[bestIdx2],bestAlpha2,str(bestLayer),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training balanced accuracy: 0.920625\n",
      "Validation balanced accuracy: 0.836667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "distEvalHype = dict(alpha = [bestAlpha2], random_state=[randState4])\n",
    "MLP1D_EvalHype =   MLPClassifier(solver='lbfgs', activation=bestActivation, hidden_layer_sizes=bestLayer3, max_iter=bestIter4)\n",
    "\n",
    "\n",
    "evalHypeParamPipe = sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=5, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= True, n_jobs = 1))\n",
    "        ])\n",
    "\n",
    "evalHypeParamPipe.fit(train_x_2,train_y_2)\n",
    "\n",
    "bestE = evalHypeParamPipe['grid_search'].best_estimator_\n",
    "pred_tr = bestE.predict(x_tr)\n",
    "pred_va = bestE.predict(x_va)\n",
    "pred_te = bestE.predict(x_te)\n",
    "\n",
    "tr_acc = balanced_accuracy_score(y_tr_df['class_uid'], pred_tr)\n",
    "va_acc = balanced_accuracy_score(y_va_df['class_uid'], pred_va)\n",
    "\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
